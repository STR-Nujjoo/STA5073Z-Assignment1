---
author: Tanweer Nujjoo
title: Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM
abstract: |
 The annual State of the Nation Address (SONA) in South Africa serves as a critical national update by the President, delivered to Parliament. The main aims of this project were to predict the presidents from their speeches, where the input datasets were prepared into 6 variations namely, bag of words (BOW), up-sampled bag of words (US BOW), down-sampled bag of words (DS BOW), TF-IDF on bag of words (TF-IDF (BOW)), up-sampled TF-IDF on bag of words (US TF-IDF (BOW)) and down-sampled TF-IDF on bag of words (DS TF-IDF (BOW)). A feedforward neural network (NN), multinomial logistic regression (MLR) and support vector machine (SVM) were utilised to model the 6 dataset variations, which led to producing 18 different models. The models were evaluated on 3 metrics namely matthews correlation coefficient (MCC), training accuracy and test accuracy. The MCC of the SVM on the BOW model was the best. The overall results were extremely poor and the NN were the worst performer. SVM and MLR had similar accuracies. All the models performed better on the non TF-IDF dataset variations and all the DS TF-IDF (BOW) had the worst accuracies. 

bibliography: bibliography.bib
format:
   html:
      toc: true
      toc-location: left
      toc-title: Contents
      embed-resources: true
      number-sections: true
      #fig-pos: '!ht'
editor: source

---

```{r libraries, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
library(stringr)
library(tidyverse)
library(tidytext)
library(textstem)
library(textdata)
library(knitr)
library(keras)
library(tensorflow)
library(caret)
library(mltools)
library(e1071)
library(glmnet)
library(xtable)
```

```{r extract, message=FALSE, warning=FALSE, results='hide', echo=FALSE}

# read in text data files and organise these into a data frame
filenames <- c('1994_post_elections_Mandela.txt', '1994_pre_elections_deKlerk.txt', '1995_Mandela.txt', '1996_Mandela.txt', '1997_Mandela.txt', '1998_Mandela.txt', 
               '1999_post_elections_Mandela.txt', '1999_pre_elections_Mandela.txt', '2000_Mbeki.txt', '2001_Mbeki.txt', '2002_Mbeki.txt', '2003_Mbeki.txt', 
               '2004_post_elections_Mbeki.txt', '2004_pre_elections_Mbeki.txt', '2005_Mbeki.txt', '2006_Mbeki.txt', '2007_Mbeki.txt', '2008_Mbeki.txt', 
               '2009_post_elections_Zuma.txt', '2009_pre_elections_Motlanthe.txt', '2010_Zuma.txt', '2011_Zuma.txt', '2012_Zuma.txt', '2013_Zuma.txt', 
               '2014_post_elections_Zuma.txt', '2014_pre_elections_Zuma.txt', '2015_Zuma.txt', '2016_Zuma.txt', '2017_Zuma.txt', '2018_Ramaphosa.txt', 
               '2019_post_elections_Ramaphosa.txt', '2019_pre_elections_Ramaphosa.txt', '2020_Ramaphosa.txt', '2021_Ramaphosa.txt', '2022_Ramaphosa.txt', '2023_Ramaphosa.txt')


this_speech <- c()
this_speech[1] <- readChar('sona-addresses-1994-2023/1994_post_elections_Mandela.txt', nchars = 27050)
this_speech[2] <- readChar('sona-addresses-1994-2023/1994_pre_elections_deKlerk.txt', nchars = 12786)
this_speech[3] <- readChar('sona-addresses-1994-2023/1995_Mandela.txt', nchars = 39019)
this_speech[4] <- readChar('sona-addresses-1994-2023/1996_Mandela.txt', nchars = 39524)
this_speech[5] <- readChar('sona-addresses-1994-2023/1997_Mandela.txt', nchars = 37489)
this_speech[6] <- readChar('sona-addresses-1994-2023/1998_Mandela.txt', nchars = 45247)
this_speech[7] <- readChar('sona-addresses-1994-2023/1999_post_elections_Mandela.txt', nchars = 34674)
this_speech[8] <- readChar('sona-addresses-1994-2023/1999_pre_elections_Mandela.txt', nchars = 41225)
this_speech[9] <- readChar('sona-addresses-1994-2023/2000_Mbeki.txt', nchars = 37552)
this_speech[10] <- readChar('sona-addresses-1994-2023/2001_Mbeki.txt', nchars = 41719)
this_speech[11] <- readChar('sona-addresses-1994-2023/2002_Mbeki.txt', nchars = 50544)
this_speech[12] <- readChar('sona-addresses-1994-2023/2003_Mbeki.txt', nchars = 58284)
this_speech[13] <- readChar('sona-addresses-1994-2023/2004_post_elections_Mbeki.txt', nchars = 34590)
this_speech[14] <- readChar('sona-addresses-1994-2023/2004_pre_elections_Mbeki.txt', nchars = 39232)
this_speech[15] <- readChar('sona-addresses-1994-2023/2005_Mbeki.txt', nchars = 54635)
this_speech[16] <- readChar('sona-addresses-1994-2023/2006_Mbeki.txt', nchars = 48643)
this_speech[17] <- readChar('sona-addresses-1994-2023/2007_Mbeki.txt', nchars = 48641)
this_speech[18] <- readChar('sona-addresses-1994-2023/2008_Mbeki.txt', nchars = 44907)
this_speech[19] <- readChar('sona-addresses-1994-2023/2009_post_elections_Zuma.txt', nchars = 31101)
this_speech[20] <- readChar('sona-addresses-1994-2023/2009_pre_elections_Motlanthe.txt', nchars = 47157)
this_speech[21] <- readChar('sona-addresses-1994-2023/2010_Zuma.txt', nchars = 26384)
this_speech[22] <- readChar('sona-addresses-1994-2023/2011_Zuma.txt', nchars = 33281)
this_speech[23] <- readChar('sona-addresses-1994-2023/2012_Zuma.txt', nchars = 33376)
this_speech[24] <- readChar('sona-addresses-1994-2023/2013_Zuma.txt', nchars = 36006)
this_speech[25] <- readChar('sona-addresses-1994-2023/2014_post_elections_Zuma.txt', nchars = 29403)
this_speech[26] <- readChar('sona-addresses-1994-2023/2014_pre_elections_Zuma.txt', nchars = 36233)
this_speech[27] <- readChar('sona-addresses-1994-2023/2015_Zuma.txt', nchars = 32860)
this_speech[28] <- readChar('sona-addresses-1994-2023/2016_Zuma.txt', nchars = 32464)
this_speech[29] <- readChar('sona-addresses-1994-2023/2017_Zuma.txt', nchars = 35981)
this_speech[30] <- readChar('sona-addresses-1994-2023/2018_Ramaphosa.txt', nchars = 33290)
this_speech[31] <- readChar('sona-addresses-1994-2023/2019_post_elections_Ramaphosa.txt', nchars = 42112)
this_speech[32] <- readChar('sona-addresses-1994-2023/2019_pre_elections_Ramaphosa.txt', nchars = 56960)
this_speech[33] <- readChar('sona-addresses-1994-2023/2020_Ramaphosa.txt', nchars = 47910)
this_speech[34] <- readChar('sona-addresses-1994-2023/2021_Ramaphosa.txt', nchars = 43352)
this_speech[35] <- readChar('sona-addresses-1994-2023/2022_Ramaphosa.txt', nchars = 52972)
this_speech[36] <- readChar('sona-addresses-1994-2023/2023_Ramaphosa.txt', nchars = 53933)

sona <- data.frame(filename = filenames, speech = this_speech, stringsAsFactors = FALSE)

```

```{r Main data cleaning, message=FALSE, warning=FALSE, results='hide', echo=FALSE}

# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$president_13 <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")

# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'

sona <-sona %>%
  mutate(speech = str_replace_all(speech, replace_reg , ' ')
         ,date = str_sub(speech, start=1, end=30)
         ,date = str_replace_all(date, "February", "02")
         ,date = str_replace_all(date, "June", "06")
         ,date = str_replace_all(date, "Feb", "02")
         ,date = str_replace_all(date, "May", "05")
         ,date = str_replace_all(date, "Jun", "06")
         ,date = str_replace_all(date, "Thursday, ","")
         ,date = str_replace_all(date, ' ', '-')        
         ,date = str_replace_all(date, "[A-z]",'')
         ,date = str_replace_all(date, '-----', '')
         ,date = str_replace_all(date, '----', '')
         ,date = str_replace_all(date, '---', '')
         ,date = str_replace_all(date, '--', '')
  )

sona <- as_tibble(sona) # convert dataframe to tibble
sona <- sona[,-1] # removing first column

unnest_reg <- "[^\\w_#@']"

```


# Introduction 

The State of the Nation Address (SONA) of the President of South Africa is an annual event in which the President of South Africa reports on the status of the nation, normally to the resumption of a joint sitting of Parliament. Various key issues are addressed in present time plus other strategies and priorities to solve related matters for the upcoming year. So, in years that elections took place, a SONA happens twice, once before and again after the election. The data obtained for this task contained the full text of all SONA speeches, from 1994 through to 2023 (see @sec-SONA_overview for an overview of the dataset).


## Objectives

**The main goals for this project are:**

* To construct at least three predictive models that take a sentence of text as input and return a prediction of which president was the source of that sentence. 

* To apply the predictive models on a variety of data manipulated models such as bag of words (BOW) and term frequency-inverse document frequency (TF-IDF).

* To evaluate each predictive model while performing a comparative analysis. 

# Materials and Methods

This section describes thoroughly the procedures undertaken to conduct the predictive modelling using the content of speeches. The entire implementation of this task was performed using RStudio, therefore the functions and libraries used throughout the process corresponds to R. However, the equivalence of this analysis can definitely be replicated on other programming platforms. There are various ways to wrangle data and only the "not so obvious" functions were explained in this report. It should be further noted that all the plots produced in this report were generated either via the `plot()` function from `base R` or the `ggplot()` function from the `ggplot2` library.

## Brief Overview of SONA dataset {#sec-SONA_overview}

Since 1994 until 2023, South Africa has had 6 presidents governing the country namely, Mandela [1994-1999], deKlerk [1994], Mbeki [2000-2008], Motlanthe [2009], Zuma [2009-2017], and Ramaphosa [2018-2023] accumulated 36 speeches throughout the years. As depicted in @fig-number_of_speeches_per_presidents the number of speeches are very imbalanced especially for deKlerk and Motlanthe, which explained by their 1-term presidency. Due to this fact, their speeches were **removed from the dataset** such that they would not contribute as noise in the learning process of the models.

```{r fig-number_of_speeches_per_presidents, message=FALSE, warning=FALSE, results='hide', echo=FALSE, fig.align='center', fig.cap="Number of speeches made by each president during the SONA events from 1994 to 2023.", fig.width = 8, fig.height = 4}

barplot(table(sona$president_13), 
        col= c("chocolate", "deeppink3", "steelblue", "darkcyan", "darkgreen", "magenta4"),
        ylab = "Number of Speeches",
        xlab = "Presidents")

```

```{r tidy sona, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
sona <- sona %>%
  filter(!president_13 %in% c("deKlerk", "Motlanthe"))
```

## Data Pre-processing & cleaning

Raw data retrieval tends to always be messy. Based on the nature of the SONA dataset retrived being semi-structured and not ideal for analysis, it was therefore, pre-processed into meaningful tabular form where each attribute (**speech**, **year**, **president**) represented a unique entity (for e.g, "*Madame Speaker and Deputy Speaker, President of the Senate and...*", *1994*, *Mandela*, respectively). That was done by extracting the years and presidents' names from the filenames using string manipulation functions. In addition, all the unnecessary regular expressions like "**(http.?(...)|(www.?(\\s|**" from the speeches were also removed. All those manipulations were done using the `stringr` library.

## Data Exploration

```{r Top 20 words used by each of the presidents, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
# PER PRESIDENT COMMON WORDS ANALYSIS -------------------------------------
# Most common words used per president ------------------------------------
common_words_Mandela <- sona %>% 
  filter(president_13 == 'Mandela') %>%
  unnest_tokens(word, speech, token = 'regex', pattern = unnest_reg, to_lower = T) %>%
  filter(str_detect(word, '[a-z]')) %>% 
  filter(!word %in% stop_words$word) %>%
  count(word, sort = T) %>%
  slice(1:20) %>%
  #filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(x = reorder(word, n), y = n, fill = -n)) + geom_col() + coord_flip() + ylab('Count') + xlab('') +
  ggtitle("Top 20 words used by Mandela") +
  theme(plot.title = element_text(size = 11), legend.position = '') +
  scale_fill_gradient(low = "deeppink3", high = "pink")

common_words_Mbeki <- sona %>% 
  filter(president_13 == 'Mbeki') %>%
  unnest_tokens(word, speech, token = 'regex', pattern = unnest_reg,  to_lower = T) %>%
  filter(str_detect(word, '[a-z]')) %>% 
  filter(!word %in% stop_words$word) %>%
  count(word, sort = T) %>%
  slice(1:20) %>%
  #filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(x = reorder(word, n), y = n, fill = -n)) + geom_col() + coord_flip() + ylab('Count') + xlab('') +
  ggtitle("Top 20 words used by Mbeki") +
  theme(plot.title = element_text(size = 11), legend.position = '')


common_words_Zuma <- sona %>% 
  filter(president_13 == 'Zuma') %>%
  unnest_tokens(word, speech, token = 'regex', pattern = unnest_reg,  to_lower = T) %>%
  filter(str_detect(word, '[a-z]')) %>% 
  filter(!word %in% stop_words$word) %>%
  count(word, sort = T) %>%
  slice(1:20) %>%
  #filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(x = reorder(word, n), y = n, fill = -n)) + geom_col() + coord_flip() + ylab('Count') + xlab('') +
  ggtitle("Top 20 words used by Zuma") +
  theme(plot.title = element_text(size = 11), legend.position = '') + 
  scale_fill_gradient(low = "magenta4", high = "plum")

common_words_Ramaphosa<- sona %>% 
  filter(president_13 == 'Ramaphosa') %>%
  unnest_tokens(word, speech, token = 'regex', pattern = unnest_reg, to_lower = T) %>%
  filter(str_detect(word, '[a-z]')) %>% 
  filter(!word %in% stop_words$word) %>%
  count(word, sort = T) %>%
  slice(1:20) %>%
  #filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(x = reorder(word, n), y = n, fill = -n)) + geom_col() + coord_flip() + ylab('Count') + xlab('') +
  ggtitle("Top 20 words used by Ramaphosa") +
  theme(plot.title = element_text(size = 11), legend.position = '') + 
  scale_fill_gradient(low = "darkgreen", high = "darkseagreen1")

```

To perform exploratory data analysis on any text related scenario, it is almost always necessary to split the words into several columns where each word or group of words becomes the attribute. This process is called **tokenisation**. Tokenisation is the split of a sequence of characters in a text by locating the word boundaries [@palmer2000tokenisation]. Now, the atomicity by which the split is performed would obviously be pursuant to a specific objective and could be in terms of per characters, per words, per n-grams, per sentences and more. As this paper did not delve deep into sentiment analysis, but rather a very general exploration, the tokenisation per words satisfied our purpose. That was simply done using the `unnest_tokens()` function from the `tidytext` library. On a side note, n-gram is a terminology very well known in the world of natural language processing (NLP), and it simply refers to a sequence of n words. If n=1, it is referred to as a unigram, if n=2, it is referred to as a bigram and if n=3, it is referred to as a trigram.

To find the top 20 words used per president, each president's name was filtered, followed by a 'word' tokenisation, lower case string detection using the matching pattern '[a-z]', exclusion of stop words (i.e, prepositions and connecting words) from the SMART lexicon of the `tidytext` library and of course a descending count and slice of the top 20 words. This process resulted as @fig-top_20_words_PP.

```{r fig-top_20_words_PP, message=FALSE, warning=FALSE, results='hide', echo=FALSE, fig.align='center', fig.cap="Top 20 words used by each president. **A** relates to Mandela's words, **B** relates to Mbeki's words, **C** relates to Zuma's words, and **D** relates to Ramaphosa's words.", fig.width = 10, fig.height = 7, fig.pos="h"}
# combining above plots in one figure
cowplot::plot_grid(common_words_Mandela, common_words_Mbeki,
                    common_words_Zuma, common_words_Ramaphosa,
                   nrow = 2, ncol = 2, labels = "AUTO", label_size = 10, label_x = 0) +
  theme(plot.background = element_rect(color = "black", linewidth = 2))
```


It was quite obvious that the words like government, people, public, south, national, development, african, economic and more would appear as top 20 words for all the presidents. Apart from that, based on the words of Mandela, he really demonstrated a presidency that was committed to public service, inclusivity, community building, and addressing a range of social, economic, and security-related challenges. Mandela's leadership was marked by a dedication to nation-building, reconciliation, and addressing the needs of the people. Based on the top 20 words from Mbeki, it seemed like his presidency was marked by a commitment to improving public services, addressing social issues, promoting economic growth and development, and addressing significant challenges. His top 20 words were not that meaningful because that is basically what all presidents would want to achieve in a society. Among the top 20 words of Zuma, the ones that really stood out was 'water' as during his presidency, South Africa was dealing with some water crisis. Ramaphosa's top 20 words revealed a lot about his personality. He is a businessman, so his approach as president was very business-oriented. He seemed to have prioritised economic development and social welfare during his term as president. Due to all presidents having the same goals and dealing with more or less similar unresolved issues over the years, similar words were present in their speeches which would make prediction from their speeches quite challenging.

## Data Variation Models {#sec-data_var}

Prior to creating the different data variation, a bag of 200 most frequent words was generated by first tokenised the SONA dataset into words while detecting lower case strings only, and excluding stop words, followed by a top 200 count. Then the dataset was tokenised into sentences after which a sentence ID was mutated to the dataset. The new dataset was now in the form where each row represents a sentence mentioned by a president whereby each sentence would uniquely be identified by the sentence ID. This dataset's format was the main input for its subsequent variations highlighted below.

### Bag of words (BOW) Model {#sec-BOW}

The new dataset with sentences was further tokenised into words where again the stop words were filtered out and only lower case strings were detected. The initial bag of frequent words prepared were then inner joined to the latter. That step dropped any words from each sentence with none of the 200 top words. Furthermore, the BOW model was achieved by just counting the number of times each of the top 200 words was used in each sentence. The BOW model was expectedly quite a sparse dataset where each column was represented by the bag of 200 most frequent words. One issue with that dataset was its uniformity (i.e, each president was grouped in order) due to how it was created while using function like `group_by()`. According to @row_shuffling_MEDIUM, it is necessary to shuffle the rows of the dataset so that variance is minimised and generalisability can be establised upon modelling. Therefore, the rows were randomly shuffled using the `sample()` function from `base R`.

### TF-IDF on BOW Model

As its name suggests, term frequency inverse document frequency (TF-IDF) is an approach where the values for each word in a document is computed through an inverse proportion of the frequency of the word in a particular document to the percentage of words appearance in any documents [@ramos2003using]. The mathematical formula for a TF-IDF calculation is given by: $$ w_d = f_{w,d} \times log \left(\frac{|D|}{f_{w,D}}\right) $$ where,

* $w$ = word;
* $D$ = document collection or corpus;
* $d$ = individual document, where $\{d \in D\}$;
* $f_{w,d}$ = frequency of $w$ in $d$;
* $|D|$ = size of the corpus [@ramos2003using].

The $log \left(\frac{|D|}{f_{w,D}}\right)$ lies between 1 and a very small constant $c$, implies that $w$ is relatively common over the entire collection of document while still holding some validity throughout $D$ [@ramos2003using]. So, the second data variation was created, by applying TF-IDF to the previous BOW model (annotated as TF-IDF (BOW) in subsequent sections), and in R it is as simple as to just utilise the `bind_tf_idf()` function from the `tidytext` library. Finally, once again, the rows of the dataset were randomly shuffled as argued in @sec-BOW.

### Dealing with Imbalanced Data

```{r Dataset variations, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
# Tokenise by sentence
sona_sentences <- sona %>% 
  unnest_tokens(sentence, speech, token = 'sentences', to_lower = T) %>%
  dplyr::select(sentence, president_13, year) 

# Add a sentence id column
sona_sentences$Sid <- 1:nrow(sona_sentences)

# Find 200 most frequent word
word_bag_200 <- sona %>% 
  unnest_tokens(word, speech, token = 'regex', pattern = unnest_reg, to_lower = T) %>%
  filter(stringr::str_detect(word, '[a-z]')) %>%
  filter(!word %in% stop_words$word) %>%
  count(word)%>%
  top_n(200, wt = n) %>%
  select(-n)

# Bag of words model: Calculate number of times each of top 200 words was used in each sentence
# Drop any words from each sentence with none of the 200 top words.
bow_200 <- sona_sentences %>%
  unnest_tokens(word, sentence, token = 'regex', pattern = unnest_reg, to_lower = T) %>%
  filter(!word %in% stop_words$word, str_detect(word, '[a-z]')) %>%
  inner_join(word_bag_200) %>%
  group_by(Sid, word) %>%
  count() %>%
  ungroup() %>%
  left_join(sona_sentences %>% select(president_13, Sid)) %>% # bring back president corresponding to sentence id
  pivot_wider(names_from = word, values_from = n, values_fill = 0) %>%
  mutate(Pid = as.integer(factor(president_13))-1) %>% # Mandela = 0, Mbeki = 1, Ramaphosa = 2, Zuma = 3
  select(Pid, everything()) %>%
  select(-Sid)

set.seed(1)
bow_200 <- bow_200[sample(nrow(bow_200)),] # shuffle rows to make dataset more random

# table(bow_200$president_13)
# str(bow_200)

# Upsampled bag of words model
US_bow_200 <- as_tibble(upSample(bow_200, factor(bow_200$president_13)))
set.seed(1)
US_bow_200 <- US_bow_200[sample(nrow(US_bow_200)),] # shuffle rows to make dataset more random

# table(US_bow_200$president_13)
# str(US_bow_200)
# head(US_bow_200)

# Downsampled bag of words model
DS_bow_200 <- as_tibble(downSample(bow_200, factor(bow_200$president_13)))
set.seed(1)
DS_bow_200 <- DS_bow_200[sample(nrow(DS_bow_200)),] # shuffle rows to make dataset more random
# table(DS_bow_200$president_13)
# str(DS_bow_200)

#######################################################################################################################
# Applying tf-idf on the bag of words model
tfidf_200 <- sona_sentences %>%
  unnest_tokens(word, sentence, token = 'regex', pattern = unnest_reg, to_lower = T) %>%
  filter(!word %in% stop_words$word, str_detect(word, '[a-z]')) %>%
  inner_join(word_bag_200) %>%
  group_by(Sid, word) %>%
  count() %>%
  ungroup() %>%
  left_join(sona_sentences %>% select(president_13, Sid)) %>% # bring back president corresponding to sentence id
  bind_tf_idf(word, Sid, n) %>%
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0) %>%
  mutate(Pid = as.integer(factor(president_13))-1) %>% # Mandela = 0, Mbeki = 1, Ramaphosa = 2, Zuma = 3
  select(Pid, everything()) %>%
  select(-Sid, -n, -tf, -idf)

set.seed(1)
tfidf_200 <- tfidf_200[sample(nrow(tfidf_200)),] # shuffle rows to make dataset more random


# table(tfidf_200$president_13)
# str(tfidf_200)

# Upsampled tf-idf model
US_tfidf_200 <- as_tibble(upSample(tfidf_200, factor(tfidf_200$president_13)))
set.seed(1)
US_tfidf_200 <- US_tfidf_200[sample(nrow(US_tfidf_200)),] # shuffle rows to make dataset more random
# table(US_tfidf_200$president_13)
# str(US_tfidf_200)

# Downsampled tf-idf model
DS_tfidf_200 <- as_tibble(downSample(tfidf_200, factor(tfidf_200$president_13)))
set.seed(1)
DS_tfidf_200 <- DS_tfidf_200[sample(nrow(DS_tfidf_200)),] # shuffle rows to make dataset more random
# table(DS_tfidf_200$president_13)
# str(DS_tfidf_200)
```

@tbl-BOW_TFIDF_imbalanced tabulated the totality of sentences appeared per president for both BOW and TF-IDF (BOW)
```{r tbl-BOW_TFIDF_imbalanced, echo=FALSE}
#| label: tbl-BOW_TFIDF_imbalanced
#| tbl-cap: "Extract of the number of sentences appeared per president for both BOW and TF-IDF (BOW) models."

a <- table(bow_200$president_13)
b <- table(tfidf_200$president_13)
d <- rbind(a,b)
rownames(d) <- c("BOW", "TF-IDF (BOW)")

kable(d)
```
models and we observed that in both cases the data was very imbalanced. The 2 most simplest ways to fix this problem is by applying up-sampling or down-sampling. Up-sampling is when the data in the minority classes are randomly duplicated or recreated to match the number of sample in that majority class. For instance, an up-sampled (US) version of the TF-IDF (BOW) (annotated as US TF-IDF (BOW)) model would unify all the entries of the other presidents to 10420, as Mbeki had the highest number of sentences. Similarly, down-sampling is when the data in the majority classes are randomly omitted to match the number of sample in that minority class. For instance, a down-sampled (DS) version of the BOW model (annotated as DS BOW) would unify all the entries of the other presidents to 1507, as Mandela had the lowest number of sentences. Both up-sampling and down-sampling have their pros and cons. For example, up-sampling can cause over-fitting but down-sampling can under-fitting due to limited information for model training. So, on the 2 model variations that we already had as explained in @sec-data_var, up-sampling and down-sampling have been applied on each. That led us into having 6 data model variations in total. 


## Predictive Models {#sec-PM}

The main intention behind this research was to be able to predict the presidents in question based on their speeches. Therefore, 3 robust machine learning models such as the deep feedforward neural network (NN), multinomial logistic regression (MLR) and support vector machine (SVM) were picked to execute the task. The following sections concisely explain each algorithm. Although hyper-parameter tuning is highly recommended, it was not performed due to time constraint except for the MLR model. Therefore, the hyper-parameters were chosen through multiple manual execution and evaluation of the models  (or a manual hyper-parameter tuning). Throughout the modelling process, each data variation was split into 70% training set and 30% test set. The training set was scaled separately and then the test set was scaled but based on the unscaled training data's mean and standard deviation. Additionally, for the NN application only, the dataset had to undergo through the process of one hot encoding using the `to_categorical` function, in preparation to be used with the `keras` library. Note that the explanation of each algorithm below was applied to the 6 data variations derived as explained in @sec-data_var.

### Deep feedforward Neural Network (NN)

In a nutshell, NN functions just like our brain where it takes a certain number of inputs and assign weights to each while distributing those in a network of intermediary outputs, assign further weights and repeat this process if necessary, until the final collection of decision/output is obtained. More formally, it contains of a layer of input with all the predictor variables, number of neurons for each hidden layer and the output layer containing the possible predictions. There are multiple activation functions which can be applied in the hidden layers and output layer. Initially, the NN models were executed using the rectified linear unit (ReLU) in the hidden layers but was found that the hyperbolic tanh activation function performed better. Mathematically, the hyperbolic tanh function can be expressed as: $$ f(x) = \frac{2}{(1+e^{-2x})}-1 $$ [@sharma2017activation]. Graphically, it is a sigmoid function symmetric around the origin and ranges from -1 to 1. Since we were dealing with a multi-class classification, the softmax was the appropriate activation function for the output layer. Softmax is a generalised sigmoidal function [@almurieb2020softmax] or can be seen as multiple sigmoid functions combined [@sharma2017activation]. Sigmoid function is generally used for binary classification as the function returns a value between 0 and 1 [@sharma2017activation]. Thus, its generalisability (the so-called softmax) makes it more flexible which in turn allows multi-class classification. Without any explanation, the equation for the softmax function is defined as follows: $$ \sigma(x)_i = \frac{e^{x_i}}{\sum_{l=1}^d e^{x'_l}} $$, and more information can be found in @almurieb2020softmax.

Moreover, the deep feedforward NN was applied using the `keras` library with the following steps:

* Create an empty model using the `keras_model_sequential()` function.
* Define the model:
  + The architecture contained 200 inputs, with a hidden layer of 500 units/neurons and tanh activation function followed by a      dropout regulariser at a 0.1 rate. The second hidden layer had 300 units/neurons with the same activation function and          again the dropout regulariser at the same rate. Finally, the output layer contained 4 units\neurons with a softmax              activation function.
* Compile the model using the `compile()` function:
  + At a bare minimum, the categorical crossentropy was chosen as loss function with the 'adam' optimiser at a learning rate of     0.01. The metric to be returned was selected to 'accuracy'.
* Train the model using the `fit()` function:
  + The algorithm was set to iterate for 100 epochs in small batches of 30. The validation set used was 20% with shuffle set to     true for more generalisability.
* Evaluate model using the `evaluate()` function on the test set.

### Multinomial Logistic Regression (MLR)

Logistic regression model are ideal for binary classification as it normally assumes the categorical response variable as success (1) or failure (0) [@el2012application]. A slight modification to this algorithm gives rise to the so-called multinomial logistic regression. Briefly, MLR revolves around the same idea of logarithm of the odds called logit just as logistic regression, but simply extended to handle multiple explanatory variables [@el2012application]. The MLR was quite simply implemented using the `glmnet()` function from the `glmnet` library, while invoking a lasso (L1) regularisation to minimise overfitting. Lasso regularisation can shrunk some coefficients exactly to 0 indicating their non-statistically significant nature of that predictor variable. Therefore, it automatically excludes them from the model. To find the optimal $\lambda$, a 10-fold cross validation was performed using the `cv.glmnet()` function. 

### Support Vector Machine (SVM)

In short, SVM sprouts from support vector classifiers (SVC) where the main idea is to separate various classes using decision boundaries whereby for a binary classification the SVC typically contains 2 margins. Of course, this can be uplifted to multi-class classification where the SVC would contain N number of margins/hyperplanes. SVM aims to maximise the margin so that the concept of variance-bias trade off is satisfied. That is, while a tight margin is subject to overfitting, a wide one introduces more generalisability. As opposed to SVC, SVM has an additional function which implement a kernel trick, facilitating non-linear classification by enlarging the feature space and improving computational efficiency [@james2013introduction]. SVM was applied using the `svm()` function from the `e1071` library. During the implementation of SVM, after several manual testing and evaluation we established on maintaining a cost of 1 with a radial kernel trick. According to @patle2013svm, radial kernel is one of the most popular kernal functions which informally speaking, adds a 'bump' around each data points. Mathematically, the kernel function is represented as: $$K(x,x_i) = e^{-\gamma \left\|x-x_i\right\|^2}$$, where $\gamma > 0$ [@patle2013svm].

## Performance Evaluation Metrics

Although all the models were evaluated using the conventional training and test accuracy, the imbalanced datasets were evaluated using the Matthew's Correlation Coefficient (MCC). Because, according to @Chicco_and_Jurman_2020, MCC is the most reliable metric when it comes to imbalance data, as it takes this specific nature into account. MCC is typically performed on binary classification and is quite straight forward. However, although MCC is still possible to compute for multi-class classification, its mathematical formulation is relatively slightly complicated. Let $C$ be the confusion matrix for class $k$, mathematically, the MCC for multi-class classification is computed as follows: $$ MCC = \frac{c \times s - \sum_k^K p_k \times t_k }{\sqrt{(s^2-\sum_k^K p_k^2)(s^2-\sum_k^Kt_k^2)}} $$ where,

* $c = \sum_k^K C_{kk}$, the total number of correct predictions (sum of diagonal); 
* $s = \sum_i^K \sum_j^K C_{ij}$, the total number of observations;
* $p_k = \sum_i^K C_{ki}$, the number of times the class k was predicted (column total);
* $t_k = \sum_i^K C_{ik}$, the number of times the class k truly occurred (row total) [@grandini2020metrics].

The coefficient ranges from -1 to 1 and is evaluated as the standard correlation matrix. In R, it was simply calculated using the `mcc()` function from the `mltools` library.

# Results & Discussions

## Inter-Model Comparison

```{r Data splits for all models, message=FALSE, warning=FALSE, results='hide', echo=FALSE}

# NEURAL NETWORK DATA PREP
# Fit NN on original bag of words model -----------------------------------
# Preprocessing for NN model
bow_200_target <- bow_200$Pid
bow_200_features <- as.matrix(bow_200[,-c(1:2)])

# Split data into train and test ------------------------------------------
# Determine sample size
set.seed(1)
ind <-  sample(1:2, nrow(bow_200), replace=TRUE, prob=c(0.7, 0.3))

# Split features
x_train_bow_200 <- bow_200_features[ind==1, ]
x_test_bow_200 <- bow_200_features[ind==2, ]
str(x_train_bow_200)
# Split target
y_train_bow_200 <- bow_200_target[ind==1]
y_test_bow_200 <- bow_200_target[ind==2]

# Scale dataset -----------------------------------------------------------
x_train_bow_200 <- scale(x_train_bow_200)

# Scale test data based on training data means and std devs
x_test_bow_200 <- scale(x_test_bow_200, center = attr(x_train_bow_200, "scaled:center"), 
                scale = attr(x_train_bow_200, "scaled:scale"))

# One hot encoding --------------------------------------------------------
y_train_bow_200 <- to_categorical(y_train_bow_200)
y_test_bow_200_original <- y_test_bow_200
y_test_bow_200 <- to_categorical(y_test_bow_200)

#######################################################################################################################
# Fit NN on upsampled bag of words model ----------------------------------
# Preprocessing for NN model
US_bow_200_target <- US_bow_200$Pid
US_bow_200_features <- as.matrix(US_bow_200[,-c(1:2,203)])

# Split data into train and test ------------------------------------------
# Determine sample size
set.seed(1)
ind <-  sample(1:2, nrow(US_bow_200), replace=TRUE, prob=c(0.7, 0.3))

# Split features
x_train_US_bow_200 <- US_bow_200_features[ind==1, ]
x_test_US_bow_200 <- US_bow_200_features[ind==2, ]
str(x_train_US_bow_200)
# Split target
y_train_US_bow_200 <- US_bow_200_target[ind==1]
y_test_US_bow_200 <- US_bow_200_target[ind==2]

# Scale dataset -----------------------------------------------------------
x_train_US_bow_200 <- scale(x_train_US_bow_200)

# Scale test data based on training data means and std devs
x_test_US_bow_200 <- scale(x_test_US_bow_200, center = attr(x_train_US_bow_200, "scaled:center"), 
                        scale = attr(x_train_US_bow_200, "scaled:scale"))

# One hot encoding --------------------------------------------------------
y_train_US_bow_200 <- to_categorical(y_train_US_bow_200)
y_test_US_bow_200_original <- y_test_US_bow_200
y_test_US_bow_200 <- to_categorical(y_test_US_bow_200)

#######################################################################################################################
# Fit NN on downsampled bag of words model ----------------------------------
# Preprocessing for NN model
DS_bow_200_target <- DS_bow_200$Pid
DS_bow_200_features <- as.matrix(DS_bow_200[,-c(1:2,203)])

# Split data into train and test ------------------------------------------
# Determine sample size
set.seed(1)
ind <-  sample(1:2, nrow(DS_bow_200), replace=TRUE, prob=c(0.7, 0.3))

# Split features
x_train_DS_bow_200 <- DS_bow_200_features[ind==1, ]
x_test_DS_bow_200 <- DS_bow_200_features[ind==2, ]
str(x_train_DS_bow_200)
# Split target
y_train_DS_bow_200 <- DS_bow_200_target[ind==1]
y_test_DS_bow_200 <- DS_bow_200_target[ind==2]

# Scale dataset -----------------------------------------------------------
x_train_DS_bow_200 <- scale(x_train_DS_bow_200)

# Scale test data based on training data means and std devs
x_test_DS_bow_200 <- scale(x_test_DS_bow_200, center = attr(x_train_DS_bow_200, "scaled:center"), 
                           scale = attr(x_train_DS_bow_200, "scaled:scale"))

# One hot encoding --------------------------------------------------------
y_train_DS_bow_200 <- to_categorical(y_train_DS_bow_200)
y_test_DS_bow_200_original <- y_test_DS_bow_200
y_test_DS_bow_200 <- to_categorical(y_test_DS_bow_200)

############################################################################################################################
# Fit NN on original tfidf bag of words model -----------------------------
# Preprocessing for NN model
tfidf_200_target <- tfidf_200$Pid
tfidf_200_features <- as.matrix(tfidf_200[,-c(1:2)])

# Split data into train and test ------------------------------------------
# Determine sample size
set.seed(1)
ind <-  sample(1:2, nrow(tfidf_200), replace=TRUE, prob=c(0.7, 0.3))

# Split features
x_train_tfidf_200 <- tfidf_200_features[ind==1, ]
x_test_tfidf_200 <- tfidf_200_features[ind==2, ]

# Split target
y_train_tfidf_200 <- tfidf_200_target[ind==1]
y_test_tfidf_200 <- tfidf_200_target[ind==2]

# Scale dataset -----------------------------------------------------------
x_train_tfidf_200 <- scale(x_train_tfidf_200)

# Scale test data based on training data means and std devs
x_test_tfidf_200 <- scale(x_test_tfidf_200, center = attr(x_train_tfidf_200, "scaled:center"), 
                        scale = attr(x_train_tfidf_200, "scaled:scale"))

# One hot encoding --------------------------------------------------------
y_train_tfidf_200 <- to_categorical(y_train_tfidf_200)
y_test_tfidf_200_original <- y_test_tfidf_200
y_test_tfidf_200 <- to_categorical(y_test_tfidf_200)

#######################################################################################################################
# Fit NN on upsampled tfidf bag of words model ----------------------------
# Preprocessing for NN model
US_tfidf_200_target <- US_tfidf_200$Pid
US_tfidf_200_features <- as.matrix(US_tfidf_200[,-c(1:2,203)])

# Split data into train and test ------------------------------------------
# Determine sample size
set.seed(1)
ind <-  sample(1:2, nrow(US_tfidf_200), replace=TRUE, prob=c(0.7, 0.3))

# Split features
x_train_US_tfidf_200 <- US_tfidf_200_features[ind==1, ]
x_test_US_tfidf_200 <- US_tfidf_200_features[ind==2, ]
str(x_train_US_tfidf_200)
# Split target
y_train_US_tfidf_200 <- US_tfidf_200_target[ind==1]
y_test_US_tfidf_200 <- US_tfidf_200_target[ind==2]

# Scale dataset -----------------------------------------------------------
x_train_US_tfidf_200 <- scale(x_train_US_tfidf_200)

# Scale test data based on training data means and std devs
x_test_US_tfidf_200 <- scale(x_test_US_tfidf_200, center = attr(x_train_US_tfidf_200, "scaled:center"), 
                           scale = attr(x_train_US_tfidf_200, "scaled:scale"))

# One hot encoding --------------------------------------------------------
y_train_US_tfidf_200 <- to_categorical(y_train_US_tfidf_200)
y_test_US_tfidf_200_original <- y_test_US_tfidf_200
y_test_US_tfidf_200 <- to_categorical(y_test_US_tfidf_200)

#######################################################################################################################
# Fit NN on downsampled tfidf bag of words model ----------------------------
# Preprocessing for NN model
DS_tfidf_200_target <- DS_tfidf_200$Pid
DS_tfidf_200_features <- as.matrix(DS_tfidf_200[,-c(1:2,203)])

# Split data into train and test ------------------------------------------
# Determine sample size
set.seed(1)
ind <-  sample(1:2, nrow(DS_tfidf_200), replace=TRUE, prob=c(0.7, 0.3))

# Split features
x_train_DS_tfidf_200 <- DS_tfidf_200_features[ind==1, ]
x_test_DS_tfidf_200 <- DS_tfidf_200_features[ind==2, ]
str(x_train_DS_tfidf_200)
# Split target
y_train_DS_tfidf_200 <- DS_tfidf_200_target[ind==1]
y_test_DS_tfidf_200 <- DS_tfidf_200_target[ind==2]

# Scale dataset -----------------------------------------------------------
x_train_DS_tfidf_200 <- scale(x_train_DS_tfidf_200)

# Scale test data based on training data means and std devs
x_test_DS_tfidf_200 <- scale(x_test_DS_tfidf_200, center = attr(x_train_DS_tfidf_200, "scaled:center"), 
                             scale = attr(x_train_DS_tfidf_200, "scaled:scale"))

# One hot encoding --------------------------------------------------------
y_train_DS_tfidf_200 <- to_categorical(y_train_DS_tfidf_200)
y_test_DS_tfidf_200_original <- y_test_DS_tfidf_200
y_test_DS_tfidf_200 <- to_categorical(y_test_DS_tfidf_200)

#######################################################################################################################
# MULTINOMIAL LOGISTIC REGRESSION DATA PREP
# MLR model on original bag of words model --------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(bow_200), size = nrow(bow_200)*0.7, replace = F)
train_bow_200 <- bow_200[ind,-2] # 70% training set
train_bow_200$Pid <- as.factor(train_bow_200$Pid) # convert target variable to factor for training set
test_bow_200 <- bow_200[-ind,-2] # 30% training set
test_bow_200$Pid <- as.factor(test_bow_200$Pid) # convert target variable to factor for test set

# convert predictor variables into a matrix/array as glmnet only accepts this data type as input
X_train_bow_200 <- as.matrix(train_bow_200[,-1])
# convert response variable into a matrix/array as glmnet only accepts matrix data type as input
Y_train_bow_200 <- as.matrix(train_bow_200[,1])

X_test_bow_200 <- as.matrix(test_bow_200[,-1])

##############################################################################################################################
# MLR model on upsampled bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(US_bow_200), size = nrow(US_bow_200)*0.7, replace = F)
train_US_bow_200 <- US_bow_200[ind,-c(2,203)] # 70% training set
train_US_bow_200$Pid <- as.factor(train_US_bow_200$Pid) # convert target variable to factor for training set
test_US_bow_200 <- US_bow_200[-ind,-c(2,203)] # 30% training set
test_US_bow_200$Pid <- as.factor(test_US_bow_200$Pid) # convert target variable to factor for test set

# convert predictor variables into a matrix/array as glmnet only accepts this data type as input
X_train_US_bow_200 <- as.matrix(train_US_bow_200[,-1])
# convert response variable into a matrix/array as glmnet only accepts matrix data type as input
Y_train_US_bow_200 <- as.matrix(train_US_bow_200[,1])

X_test_US_bow_200 <- as.matrix(test_US_bow_200[,-1])

##############################################################################################################################
# MLR model on downsampled bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(DS_bow_200), size = nrow(DS_bow_200)*0.7, replace = F)
train_DS_bow_200 <- DS_bow_200[ind,-c(2,203)] # 70% training set
train_DS_bow_200$Pid <- as.factor(train_DS_bow_200$Pid) # convert target variable to factor for training set
test_DS_bow_200 <- DS_bow_200[-ind,-c(2,203)] # 30% training set
test_DS_bow_200$Pid <- as.factor(test_DS_bow_200$Pid) # convert target variable to factor for test set

# convert predictor variables into a matrix/array as glmnet only accepts this data type as input
X_train_DS_bow_200 <- as.matrix(train_DS_bow_200[,-1])
# convert response variable into a matrix/array as glmnet only accepts matrix data type as input
Y_train_DS_bow_200 <- as.matrix(train_DS_bow_200[,1])

X_test_DS_bow_200 <- as.matrix(test_DS_bow_200[,-1])

##############################################################################################################################
# MLR model on original tfidf bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(tfidf_200), size = nrow(tfidf_200)*0.7, replace = F)
train_tfidf_200 <- tfidf_200[ind,-2] # 70% training set
train_tfidf_200$Pid <- as.factor(train_tfidf_200$Pid) # convert target variable to factor for training set
test_tfidf_200 <- tfidf_200[-ind,-2] # 30% training set
test_tfidf_200$Pid <- as.factor(test_tfidf_200$Pid) # convert target variable to factor for test set

# convert predictor variables into a matrix/array as glmnet only accepts this data type as input
X_train_tfidf_200 <- as.matrix(train_tfidf_200[,-1])
# convert response variable into a matrix/array as glmnet only accepts matrix data type as input
Y_train_tfidf_200 <- as.matrix(train_tfidf_200[,1])

X_test_tfidf_200 <- as.matrix(test_tfidf_200[,-1])

##############################################################################################################################
# MLR model on upsampled tfidf bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(US_tfidf_200), size = nrow(US_tfidf_200)*0.7, replace = F)
train_US_tfidf_200 <- US_tfidf_200[ind,-c(2,203)] # 70% training set
train_US_tfidf_200$Pid <- as.factor(train_US_tfidf_200$Pid) # convert target variable to factor for training set
test_US_tfidf_200 <- US_tfidf_200[-ind,-c(2,203)] # 30% training set
test_US_tfidf_200$Pid <- as.factor(test_US_tfidf_200$Pid) # convert target variable to factor for test set

# convert predictor variables into a matrix/array as glmnet only accepts this data type as input
X_train_US_tfidf_200 <- as.matrix(train_US_tfidf_200[,-1])
# convert response variable into a matrix/array as glmnet only accepts matrix data type as input
Y_train_US_tfidf_200 <- as.matrix(train_US_tfidf_200[,1])

X_test_US_tfidf_200 <- as.matrix(test_US_tfidf_200[,-1])

##############################################################################################################################
# MLR model on downsampled tfidf bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(DS_tfidf_200), size = nrow(DS_tfidf_200)*0.7, replace = F)
train_DS_tfidf_200 <- DS_tfidf_200[ind,-c(2,203)] # 70% training set
train_DS_tfidf_200$Pid <- as.factor(train_DS_tfidf_200$Pid) # convert target variable to factor for training set
test_DS_tfidf_200 <- DS_tfidf_200[-ind,-c(2,203)] # 30% training set
test_DS_tfidf_200$Pid <- as.factor(test_DS_tfidf_200$Pid) # convert target variable to factor for test set

# convert predictor variables into a matrix/array as glmnet only accepts this data type as input
X_train_DS_tfidf_200 <- as.matrix(train_DS_tfidf_200[,-1])
# convert response variable into a matrix/array as glmnet only accepts matrix data type as input
Y_train_DS_tfidf_200 <- as.matrix(train_DS_tfidf_200[,1])

X_test_DS_tfidf_200 <- as.matrix(test_DS_tfidf_200[,-1])

##############################################################################################################################
# SUPPORT VECTOR MACHINE DATA PREP
# SVM model on original bag of words model --------------------------------
set.seed(1)
ind <- sample(1:nrow(bow_200), size = nrow(bow_200)*0.7, replace = F)
train_bow_200 <- bow_200[ind,-2] # 70% training set
train_bow_200$Pid <- as.factor(train_bow_200$Pid) # convert target variable to factor for training set
test_bow_200 <- bow_200[-ind,-2] # 30% training set
test_bow_200$Pid <- as.factor(test_bow_200$Pid) # convert target variable to factor for test set

##############################################################################################################################
# SVM model on upsampled bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(US_bow_200), size = nrow(US_bow_200)*0.7, replace = F)
train_US_bow_200 <- US_bow_200[ind,-c(2,203)] # 70% training set
train_US_bow_200$Pid <- as.factor(train_US_bow_200$Pid) # convert target variable to factor for training set
test_US_bow_200 <- US_bow_200[-ind,-c(2,203)] # 30% training set
test_US_bow_200$Pid <- as.factor(test_US_bow_200$Pid) # convert target variable to factor for test set

##############################################################################################################################
# SVM model on downsampled bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(DS_bow_200), size = nrow(DS_bow_200)*0.7, replace = F)
train_DS_bow_200 <- DS_bow_200[ind,-c(2,203)] # 70% training set
train_DS_bow_200$Pid <- as.factor(train_DS_bow_200$Pid) # convert target variable to factor for training set
test_DS_bow_200 <- DS_bow_200[-ind,-c(2,203)] # 30% training set
test_DS_bow_200$Pid <- as.factor(test_DS_bow_200$Pid) # convert target variable to factor for test set

##############################################################################################################################
# SVM model on original tfidf bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(tfidf_200), size = nrow(tfidf_200)*0.7, replace = F)
train_tfidf_200 <- tfidf_200[ind,-2] # 70% training set
train_tfidf_200$Pid <- as.factor(train_tfidf_200$Pid) # convert target variable to factor for training set
test_tfidf_200 <- tfidf_200[-ind,-2] # 30% training set
test_tfidf_200$Pid <- as.factor(test_tfidf_200$Pid) # convert target variable to factor for test set

##############################################################################################################################
# SVM model on upsampled tfidf bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(US_tfidf_200), size = nrow(US_tfidf_200)*0.7, replace = F)
train_US_tfidf_200 <- US_tfidf_200[ind,-c(2,203)] # 70% training set
train_US_tfidf_200$Pid <- as.factor(train_US_tfidf_200$Pid) # convert target variable to factor for training set
test_US_tfidf_200 <- US_tfidf_200[-ind,-c(2,203)] # 30% training set
test_US_tfidf_200$Pid <- as.factor(test_US_tfidf_200$Pid) # convert target variable to factor for test set

##############################################################################################################################
# SVM model on downsampled tfidf bag of words model -------------------------------
# randomly splitting data
set.seed(1)
ind <- sample(1:nrow(DS_tfidf_200), size = nrow(DS_tfidf_200)*0.7, replace = F)
train_DS_tfidf_200 <- DS_tfidf_200[ind,-c(2,203)] # 70% training set
train_DS_tfidf_200$Pid <- as.factor(train_DS_tfidf_200$Pid) # convert target variable to factor for training set
test_DS_tfidf_200 <- DS_tfidf_200[-ind,-c(2,203)] # 30% training set
test_DS_tfidf_200$Pid <- as.factor(test_DS_tfidf_200$Pid) # convert target variable to factor for test set

```

```{r Neural network skeletons, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
# Neural Network Model ----------------------------------------------------
set.seed(1)
# create empty model
model_bow_200 <- keras_model_sequential()

# define model
model_bow_200 %>% 
  layer_dense(units = 500, activation = 'tanh', input_shape = c(200)) %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 300, activation = 'tanh') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 4, activation = 'softmax')

#summary(model_bow_200)

# compile model
model_bow_200 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('accuracy'),
)

#################
# Neural Network Model ----------------------------------------------------
set.seed(1)
# create empty model
model_US_bow_200 <- keras_model_sequential()

# define model
model_US_bow_200 %>% 
  layer_dense(units = 500, activation = 'tanh', input_shape = c(200)) %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 300, activation = 'tanh') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 4, activation = 'softmax')

#summary(model_US_bow_200)

# compile model
model_US_bow_200 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('accuracy'),
)
########################
# Neural Network Model ----------------------------------------------------
set.seed(1)
# create empty model
model_DS_bow_200 <- keras_model_sequential()

# define model
model_DS_bow_200 %>% 
  layer_dense(units = 500, activation = 'tanh', input_shape = c(200)) %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 300, activation = 'tanh') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 4, activation = 'softmax')

#summary(model_DS_bow_200)

# compile model
model_DS_bow_200 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('accuracy'),
)
#####################
# Neural Network Model ----------------------------------------------------
set.seed(1)
# create empty model
model_tfidf_200 <- keras_model_sequential()

# define model
model_tfidf_200 %>% 
  layer_dense(units = 500, activation = 'tanh', input_shape = c(200)) %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 300, activation = 'tanh') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 4, activation = 'softmax')

#summary(model_tfidf_200)

# compile model
model_tfidf_200 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('accuracy'),
)

################
# Neural Network Model ----------------------------------------------------
set.seed(1)
# create empty model
model_US_tfidf_200 <- keras_model_sequential()

# define model
model_US_tfidf_200 %>% 
  layer_dense(units = 500, activation = 'tanh', input_shape = c(200)) %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 300, activation = 'tanh') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 4, activation = 'softmax')

#summary(model_US_tfidf_200)

# compile model
model_US_tfidf_200 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('accuracy'),
)
################
# Neural Network Model ----------------------------------------------------
set.seed(1)
# create empty model
model_DS_tfidf_200 <- keras_model_sequential()

# define model
model_DS_tfidf_200 %>% 
  layer_dense(units = 500, activation = 'tanh', input_shape = c(200)) %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 300, activation = 'tanh') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 4, activation = 'softmax')

#summary(model_DS_tfidf_200)

# compile model
model_DS_tfidf_200 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('accuracy'),
)

```

```{r Load all 18 models and SVM predictions, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
set.seed(1)
load("Rdata/NN_bow_200.Rdata")
load("Rdata/NN_US_bow_200.Rdata")
load("Rdata/NN_DS_bow_200.Rdata")
load("Rdata/NN_tfidf_200.Rdata")
load("Rdata/NN_US_tfidf_200.Rdata")
load("Rdata/NN_DS_tfidf_200.Rdata")
load("Rdata/MLR_bow_200.Rdata")
load("Rdata/MLR_US_bow_200.Rdata")
load("Rdata/MLR_DS_bow_200.Rdata")
load("Rdata/MLR_tfidf_200.Rdata")
load("Rdata/MLR_US_tfidf_200.Rdata")
load("Rdata/MLR_DS_tfidf_200.Rdata")
load("Rdata/SVM_bow_200.Rdata")
load("Rdata/SVM_US_bow_200.Rdata")
load("Rdata/SVM_DS_bow_200.Rdata")
load("Rdata/SVM_tfidf_200.Rdata")
load("Rdata/SVM_US_tfidf_200.Rdata")
load("Rdata/SVM_DS_tfidf_200.Rdata")

load("Rdata/SVM_predictions.Rdata")
```

```{r Evalution metrics of all the models, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
# NEURAL NETWORK EVALUATION METRICS
############# 1
#plot(history_bow_200)
NN_bow_200_train_accuracy <- mean(history_bow_200$metrics$accuracy) # training accuracy of NN on original BOW model
NN_bow_200_val_accuracy <- mean(history_bow_200$metrics$val_accuracy) # validation accuracy of NN on original BOW model
# model evaluation
NN_bow_200_test_accuracy <- model_bow_200 %>% evaluate(x_test_bow_200, y_test_bow_200) # test accuracy of NN on original BOW model


# confusion matrix
y_test_bow_200_hat <- model_bow_200 %>% predict(x_test_bow_200) %>% k_argmax() %>% as.numeric()
#table(y_test_bow_200_original, y_test_bow_200_hat)

# Matthew's correlation coefficient
MCC_bow_200 <- mcc(preds = y_test_bow_200_hat, actuals = y_test_bow_200_original)

############# 2
#plot(history_US_bow_200)
NN_US_bow_200_train_accuracy <- mean(history_US_bow_200$metrics$accuracy) # training accuracy of NN on upsampled BOW model
NN_US_bow_200_val_accuracy <- mean(history_US_bow_200$metrics$val_accuracy) # validation accuracy of NN on upsampled BOW model

# model evaluation
NN_US_bow_200_test_accuracy <- model_US_bow_200 %>% evaluate(x_test_US_bow_200, y_test_US_bow_200) # test accuracy of NN on upsampled BOW model

# confusion matrix
y_test_US_bow_200_hat <- model_US_bow_200 %>% predict(x_test_US_bow_200) %>% k_argmax() %>% as.numeric()
#table(y_test_US_bow_200_original, y_test_US_bow_200_hat)

############# 3
#plot(history_DS_bow_200)
NN_DS_bow_200_train_accuracy <- mean(history_DS_bow_200$metrics$accuracy) # training accuracy of NN on downsampled BOW model
NN_DS_bow_200_val_accuracy <- mean(history_DS_bow_200$metrics$val_accuracy)  # validation accuracy of NN on downsampled BOW model

# model evaluation
NN_DS_bow_200_test_accuracy <- model_DS_bow_200 %>% evaluate(x_test_DS_bow_200, y_test_DS_bow_200) # test accuracy of NN on downsampled BOW model

# confusion matrix
y_test_DS_bow_200_hat <- model_DS_bow_200 %>% predict(x_test_DS_bow_200) %>% k_argmax() %>% as.numeric()
#table(y_test_DS_bow_200_original, y_test_DS_bow_200_hat)

############# 4
#plot(history_tfidf_200)
NN_tfidf_200_train_accuracy <- mean(history_tfidf_200$metrics$accuracy) # training accuracy of NN on original tfidf model
NN_tfidf_200_val_accuracy <- mean(history_tfidf_200$metrics$val_accuracy) # validation accuracy of NN on original tfidf model


# model evaluation
NN_tfidf_200_test_accuracy <- model_tfidf_200 %>% evaluate(x_test_tfidf_200, y_test_tfidf_200) # test accuracy of NN on original tfidf model

# confusion matrix
y_test_tfidf_200_hat <- model_tfidf_200 %>% predict(x_test_tfidf_200) %>% k_argmax() %>% as.numeric()
#table(y_test_tfidf_200_original, y_test_tfidf_200_hat)

# Matthew's correlation coefficient
MCC_tfidf_200 <- mcc(preds = y_test_tfidf_200_hat, actuals = y_test_tfidf_200_original)

############# 5
#plot(history_US_tfidf_200)
NN_US_tfidf_200_train_accuracy <- mean(history_US_tfidf_200$metrics$accuracy) # training accuracy of NN on upsampled tfidf model
NN_US_tfidf_200_val_accuracy <- mean(history_US_tfidf_200$metrics$val_accuracy) # validation accuracy of NN on upsampled tfidf model

# model evaluation
NN_US_tfidf_200_test_accuracy <- model_US_tfidf_200 %>% evaluate(x_test_US_tfidf_200, y_test_US_tfidf_200) # test accuracy of NN on upsampled tfidf model

# confusion matrix
y_test_US_tfidf_200_hat <- model_US_tfidf_200 %>% predict(x_test_US_tfidf_200) %>% k_argmax() %>% as.numeric()
#table(y_test_US_tfidf_200_original, y_test_US_tfidf_200_hat)

############# 6
#plot(history_DS_tfidf_200)
NN_DS_tfidf_200_train_accuracy <- mean(history_DS_tfidf_200$metrics$accuracy) # training accuracy of NN on downsampled tfidf model
NN_DS_tfidf_200_val_accuracy <- mean(history_DS_tfidf_200$metrics$val_accuracy) # validation accuracy of NN on downsampled tfidf model

# model evaluation
NN_DS_tfidf_200_test_accuracy <- model_DS_tfidf_200 %>% evaluate(x_test_DS_tfidf_200, y_test_DS_tfidf_200) # test accuracy of NN on upsampled tfidf model

# confusion matrix
y_test_DS_tfidf_200_hat <- model_DS_tfidf_200 %>% predict(x_test_DS_tfidf_200) %>% k_argmax() %>% as.numeric()
#table(y_test_DS_tfidf_200_original, y_test_DS_tfidf_200_hat)

################################################################################
# MULTINOMIAL LOGISTIC REGRESSION EVALUATION METRICS
############# 1
mlr_bow_200_train_accuracy <- mean(predict(bow_200_mlr, 
                                     newx = X_train_bow_200, 
                                     s = cv_lasso_bow_200_mlr$lambda.min, 
                                     type = 'class') == train_bow_200$Pid) # training accuracy

mlr_bow_200_test_accuracy <- mean(predict(bow_200_mlr, 
                                           newx = X_test_bow_200, 
                                           s = cv_lasso_bow_200_mlr$lambda.min, 
                                           type = 'class') == test_bow_200$Pid) # test accuracy

# Matthew's correlation coefficient
bow_200_mlr_mcc <- mcc(preds = as.numeric(predict(bow_200_mlr, 
                                       newx = X_test_bow_200, 
                                       s = cv_lasso_bow_200_mlr$lambda.min, 
                                       type = 'class')),
                       actuals = as.numeric(test_bow_200$Pid)) # MCC
############# 2
mlr_US_bow_200_train_accuracy <- mean(predict(US_bow_200_mlr, 
                                           newx = X_train_US_bow_200, 
                                           s = cv_lasso_US_bow_200_mlr$lambda.min, 
                                           type = 'class') == train_US_bow_200$Pid) # training accuracy

mlr_US_bow_200_test_accuracy <- mean(predict(US_bow_200_mlr, 
                                          newx = X_test_US_bow_200, 
                                          s = cv_lasso_US_bow_200_mlr$lambda.min, 
                                          type = 'class') == test_US_bow_200$Pid) # test accuracy

############# 3
mlr_DS_bow_200_train_accuracy <- mean(predict(DS_bow_200_mlr, 
                                              newx = X_train_DS_bow_200, 
                                              s = cv_lasso_DS_bow_200_mlr$lambda.min, 
                                              type = 'class') == train_DS_bow_200$Pid) # training accuracy

mlr_DS_bow_200_test_accuracy <- mean(predict(DS_bow_200_mlr, 
                                             newx = X_test_DS_bow_200, 
                                             s = cv_lasso_DS_bow_200_mlr$lambda.min, 
                                             type = 'class') == test_DS_bow_200$Pid) # test accuracy

############# 4
mlr_tfidf_200_train_accuracy <- mean(predict(tfidf_200_mlr, 
                                           newx = X_train_tfidf_200, 
                                           s = cv_lasso_tfidf_200_mlr$lambda.min, 
                                           type = 'class') == train_tfidf_200$Pid) # training accuracy

mlr_tfidf_200_test_accuracy <- mean(predict(tfidf_200_mlr, 
                                          newx = X_test_tfidf_200, 
                                          s = cv_lasso_tfidf_200_mlr$lambda.min, 
                                          type = 'class') == test_tfidf_200$Pid) # test accuracy

# Matthew's correlation coefficient
tfidf_200_mlr_mcc <- mcc(preds = as.numeric(predict(tfidf_200_mlr, 
                                                  newx = X_test_tfidf_200, 
                                                  s = cv_lasso_tfidf_200_mlr$lambda.min, 
                                                  type = 'class')),
                       actuals = as.numeric(test_tfidf_200$Pid)) # MCC

############# 5
mlr_US_tfidf_200_train_accuracy <- mean(predict(US_tfidf_200_mlr, 
                                              newx = X_train_US_tfidf_200, 
                                              s = cv_lasso_US_tfidf_200_mlr$lambda.min, 
                                              type = 'class') == train_US_tfidf_200$Pid) # training accuracy

mlr_US_tfidf_200_test_accuracy <- mean(predict(US_tfidf_200_mlr, 
                                             newx = X_test_US_tfidf_200, 
                                             s = cv_lasso_US_tfidf_200_mlr$lambda.min, 
                                             type = 'class') == test_US_tfidf_200$Pid) # test accuracy

############# 6
mlr_DS_tfidf_200_train_accuracy <- mean(predict(DS_tfidf_200_mlr, 
                                                newx = X_train_DS_tfidf_200, 
                                                s = cv_lasso_DS_tfidf_200_mlr$lambda.min, 
                                                type = 'class') == train_DS_tfidf_200$Pid) # training accuracy

mlr_DS_tfidf_200_test_accuracy <- mean(predict(DS_tfidf_200_mlr, 
                                               newx = X_test_DS_tfidf_200, 
                                               s = cv_lasso_DS_tfidf_200_mlr$lambda.min, 
                                               type = 'class') == test_DS_tfidf_200$Pid) # test accuracy

################################################################################
# SUPPORT VECTOR MACHINE EVALUATION METRICS
############# 1
svm_bow_200_train_accuracy <- mean(svm_bow_200_trainpred == train_bow_200$Pid)
svm_bow_200_test_accuracy <- mean(svm_bow_200_testpred == test_bow_200$Pid)
svm_bow_200_mcc <-  mcc(preds = as.numeric(svm_bow_200_testpred),
                        actuals = as.numeric(test_bow_200$Pid))
############# 2
svm_US_bow_200_train_accuracy <- mean(svm_US_bow_200_trainpred == train_US_bow_200$Pid)
svm_US_bow_200_test_accuracy <- mean(svm_US_bow_200_testpred == test_US_bow_200$Pid)

############# 3
svm_DS_bow_200_train_accuracy <- mean(svm_DS_bow_200_trainpred == train_DS_bow_200$Pid)
svm_DS_bow_200_test_accuracy <- mean(svm_DS_bow_200_testpred == test_DS_bow_200$Pid)

############# 4
svm_tfidf_200_train_accuracy <- mean(svm_tfidf_200_trainpred == train_tfidf_200$Pid)
svm_tfidf_200_test_accuracy <- mean(svm_tfidf_200_testpred == test_tfidf_200$Pid)
svm_tfidf_200_mcc <-  mcc(preds = as.numeric(svm_tfidf_200_testpred),
                        actuals = as.numeric(test_tfidf_200$Pid))
############# 5
svm_US_tfidf_200_train_accuracy <- mean(svm_US_tfidf_200_trainpred == train_US_tfidf_200$Pid)
svm_US_tfidf_200_test_accuracy <- mean(svm_US_tfidf_200_testpred == test_US_tfidf_200$Pid)

############# 6
svm_DS_tfidf_200_train_accuracy <- mean(svm_DS_tfidf_200_trainpred == train_DS_tfidf_200$Pid)
svm_DS_tfidf_200_test_accuracy <- mean(svm_DS_tfidf_200_testpred == test_DS_tfidf_200$Pid)

```

```{r evaluation_metrics_df, message=FALSE, warning=FALSE, results='hide', echo=FALSE}

Model <-  c("NN<sub>tanh+softmax</sub>: Original BOW", "NN<sub>tanh+softmax</sub>: US BOW", "NN<sub>tanh+softmax</sub>: DS BOW", "NN<sub>tanh+softmax</sub>: Original TF-IDF (BOW)", "NN<sub>tanh+softmax</sub>: US TF-IDF (BOW)", "NN<sub>tanh+softmax</sub>: DS TF-IDF (BOW)", " ",
                                "MLR<sub>L1</sub>: Original BOW", "MLR<sub>L1</sub>: US BOW","MLR<sub>L1</sub>: DS BOW", "MLR<sub>L1</sub>: Original TF-IDF (BOW)", "MLR<sub>L1</sub>: US TF-IDF (BOW)", "MLR<sub>L1</sub>: DS TF-IDF (BOW)", " ",
                                "SVM<sub>radial</sub>: Original BOW", "SVM<sub>radial</sub>: US BOW", "SVM<sub>radial</sub>: DS BOW", "SVM<sub>radial</sub>: Original TF-IDF (BOW)", "SVM<sub>radial</sub>: US TF-IDF (BOW)", "SVM<sub>radial</sub>: DS TF-IDF (BOW)")

MCC <-  c(round(MCC_bow_200, 3), "-", "-", round(MCC_tfidf_200,3), "-", "-", " ", round(bow_200_mlr_mcc,3), "-", "-", round(tfidf_200_mlr_mcc, 3), "-", "-", " ", round(svm_bow_200_mcc,3), "-", "-", round(svm_tfidf_200_mcc,3), "-", "-")

Train_acc <- c(round(NN_bow_200_train_accuracy,3), round(NN_US_bow_200_train_accuracy,3), round(NN_DS_bow_200_train_accuracy,3), round(NN_tfidf_200_train_accuracy,3), round(NN_US_tfidf_200_train_accuracy,3), round(NN_DS_tfidf_200_train_accuracy,3), " ", round(mlr_bow_200_train_accuracy,3), round(mlr_US_bow_200_train_accuracy,3), round(mlr_DS_bow_200_train_accuracy,3), round(mlr_tfidf_200_train_accuracy,3), round(mlr_US_tfidf_200_train_accuracy,3), round(mlr_DS_tfidf_200_train_accuracy,3), " ", round(svm_bow_200_train_accuracy,3), round(svm_US_bow_200_train_accuracy,3), round(svm_DS_bow_200_train_accuracy,3), round(svm_tfidf_200_train_accuracy,3), round(svm_US_tfidf_200_train_accuracy,3), round(svm_DS_tfidf_200_train_accuracy,3))
           
Test_acc <- c(round(NN_bow_200_test_accuracy[[2]],3), round(NN_US_bow_200_test_accuracy[[2]],3), round(NN_DS_bow_200_test_accuracy[[2]],3), round(NN_tfidf_200_test_accuracy[[2]],3), round(NN_US_tfidf_200_test_accuracy[[2]],3), round(NN_DS_tfidf_200_test_accuracy[[2]],3), " ", round(mlr_bow_200_test_accuracy,3), round(mlr_US_bow_200_test_accuracy,3), round(mlr_DS_bow_200_test_accuracy,3), round(mlr_tfidf_200_test_accuracy,3), round(mlr_US_tfidf_200_test_accuracy,3), round(mlr_DS_tfidf_200_test_accuracy,3), " ", round(svm_bow_200_test_accuracy,3), round(svm_US_bow_200_test_accuracy,3), round(svm_DS_bow_200_test_accuracy,3),
round(svm_tfidf_200_test_accuracy,3), round(svm_US_tfidf_200_test_accuracy,3), round(svm_DS_tfidf_200_test_accuracy,3))

results <- cbind(Model, MCC, Train_acc, Test_acc)
colnames(results) <- c("Model", "MCC", "Training Accuracy", "Test Accuracy")


```

The implementation of the 3 machine learning algorithms discussed in @sec-PM on each of the 6 data variations (see @sec-data_var), resulted in 18 outcomes as shown in @tbl-evaluation_metrics_table. A first glimpse on the overall performance metrics, revealed extremely poor accuracies. In fact all the test accuracies were
```{r tbl-evaluation_metrics_table, echo=FALSE}
#| label: tbl-evaluation_metrics_table
#| tbl-cap: "Model performance in predicting the presidents from their speeches using 200 predictor variables. Metrics reported on 18 different models are MCC, training accuracy and test accuracy. Empty cells for MCC is due to application irrelevancy- classes are fully balanced."
#| tbl-colwidths: [40, 20, 20, 20]
kable(results, align = "lccc")
```
below 50% and the MCCs also were quite low. In addition, only 5 training accuracies were above 70%. For the unbalanced data, SVM has definitely demonstrated as best candidate due to its relatively high MCC (`r round(svm_bow_200_mcc,3)`). Although, the best performer according to training accuracy kept on altering among each group of models, the models applied on the DS TF-IDF (BOW) were consistently the worst throughout each group of models. Regarding the test accuracy, the family of NN showed very poor results and for both the MLR and SVM, the test accuracy were around the same values. Throughout all the models applied, NN was definitely the poorest performer.

## Intra-Model Comparison

Among the family of NN models, the NN applied on the original BOW and the original TF-IDF (BOW) were both extremely poor with MCCs of `r round(MCC_bow_200, 3)` and `r round(MCC_tfidf_200,3)`, respectively. Albeit, the NN applied on the DS BOW having the highest training accuracy of `r round(NN_DS_bow_200_train_accuracy,3)`, its test accuracy (`r round(NN_DS_bow_200_test_accuracy[[2]],3)`) was the poorest. This is a very good example showing the complexity of the model being high, biased and therefore obtaining a very good training accuracy. Due to its poor biased-variance trade off, in this case overfitting on the training set, the test error became really high on the unseen test data. Surprisingly, the NN model applied on the DS TF-IDF with the lowest training accuracy (`r round(NN_DS_tfidf_200_train_accuracy,3)`) had relatively one of the highest test accuracy (`r round(NN_DS_tfidf_200_test_accuracy[[2]],3)`).

Amid the family of MLR models, the MLR applied on the original BOW and the original TF-IDF (BOW) were even worse with MCCs of `r round(bow_200_mlr_mcc,3)` and `r round(tfidf_200_mlr_mcc, 3)`, respectively. However, the MLR applied on the original BOW had relatively the largest training and test accuracy  of `r round(mlr_bow_200_train_accuracy,3)` and `r round(mlr_bow_200_test_accuracy,3)`, respectively. Furthermore, the MLR on the US TF-IDF (BOW) had the lowest performance with a training accuracy of `r round(mlr_US_tfidf_200_train_accuracy,3)`. It could also be observed that the training and test accuracies were fairly consistent among each other, meaning that the biased-variance trade off was quite balanced. 

Last but not least, within the family of SVM models, the SVM applied on the original BOW had a better MCC (`r round(svm_bow_200_mcc,3)`) than the original TF-IDF (BOW) one (`r round(svm_tfidf_200_mcc,3)`). SVM onto the DS BOW dataset highlighted the highest training accuracy of `r round(svm_DS_bow_200_train_accuracy,3)` which pertained to a test accuracy of `r round(svm_DS_bow_200_test_accuracy,3)`. The lowest training and test accuracy were `r round(svm_DS_tfidf_200_train_accuracy,3)` and `r round(svm_DS_tfidf_200_test_accuracy,3)`, respectively, for the SVM applied on DS TF-IDF (BOW). The non TF-IDF dataset seemed to have performed better both in terms of training and test accuracy.

# Conclusion

The prediction of presidents based on their speeches revealed several important findings. Firstly, data variations fed into the models have quite an impact on the predictive outcomes. Dealing with imbalanced data by applying up-sampling or down-sampling strategies do not always ensure good results upon performance evaluation. Although, all the machine learning models used in this project are generally claimed to be robust, all of them failed to produce a robust model to predict the presidents from their speeches. The low accuracies throughout mean that perhaps these models were not appropriate for the variations of this dataset or perhaps the variations of the SONA dataset were not at their best state yet. Moreover, SVM and MLR showed very similar performance both in terms of training and test accuracy. The NNs had the worst test accuracies throughout.

# Recommendations

Look into developing new strategies for the data variations so that the predictions can improve. Embark on more advanced deep learning algorithms such as convolution neural network (CNN) or recurrent neural network (RNN) for an even more robust model. Lastly, find algorithms which can deal with sparse dataset fairly well.

# References {.unnumbered}



