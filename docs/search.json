[
  {
    "objectID": "Assignment1_Tanweer.html#objectives",
    "href": "Assignment1_Tanweer.html#objectives",
    "title": "Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM",
    "section": "1.1 Objectives",
    "text": "1.1 Objectives\nThe main goals for this project are:\n\nTo construct at least three predictive models that take a sentence of text as input and return a prediction of which president was the source of that sentence.\nTo apply the predictive models on a variety of data manipulated models such as bag of words (BOW) and term frequency-inverse document frequency (TF-IDF).\nTo evaluate each predictive model while performing a comparative analysis."
  },
  {
    "objectID": "Assignment1_Tanweer.html#sec-SONA_overview",
    "href": "Assignment1_Tanweer.html#sec-SONA_overview",
    "title": "Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM",
    "section": "2.1 Brief Overview of SONA dataset",
    "text": "2.1 Brief Overview of SONA dataset\nSince 1994 until 2023, South Africa has had 6 presidents governing the country namely, Mandela [1994-1999], deKlerk [1994], Mbeki [2000-2008], Motlanthe [2009], Zuma [2009-2017], and Ramaphosa [2018-2023] accumulated 36 speeches throughout the years. As depicted in Figure 1 the number of speeches are very imbalanced especially for deKlerk and Motlanthe, which explained by their 1-term presidency. Due to this fact, their speeches were removed from the dataset such that they would not contribute as noise in the learning process of the models.\n\n\n\n\n\nFigure 1: Number of speeches made by each president during the SONA events from 1994 to 2023."
  },
  {
    "objectID": "Assignment1_Tanweer.html#data-pre-processing-cleaning",
    "href": "Assignment1_Tanweer.html#data-pre-processing-cleaning",
    "title": "Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM",
    "section": "2.2 Data Pre-processing & cleaning",
    "text": "2.2 Data Pre-processing & cleaning\nRaw data retrieval tends to always be messy. Based on the nature of the SONA dataset retrived being semi-structured and not ideal for analysis, it was therefore, pre-processed into meaningful tabular form where each attribute (speech, year, president) represented a unique entity (for e.g, “Madame Speaker and Deputy Speaker, President of the Senate and…”, 1994, Mandela, respectively). That was done by extracting the years and presidents’ names from the filenames using string manipulation functions. In addition, all the unnecessary regular expressions like “(http.?(…)|(www.?(\\s|” from the speeches were also removed. All those manipulations were done using the stringr library."
  },
  {
    "objectID": "Assignment1_Tanweer.html#data-exploration",
    "href": "Assignment1_Tanweer.html#data-exploration",
    "title": "Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM",
    "section": "2.3 Data Exploration",
    "text": "2.3 Data Exploration\n\n\n\nTo perform exploratory data analysis on any text related scenario, it is almost always necessary to split the words into several columns where each word or group of words becomes the attribute. This process is called tokenisation. Tokenisation is the split of a sequence of characters in a text by locating the word boundaries (Palmer 2000). Now, the atomicity by which the split is performed would obviously be pursuant to a specific objective and could be in terms of per characters, per words, per n-grams, per sentences and more. As this paper did not delve deep into sentiment analysis, but rather a very general exploration, the tokenisation per words satisfied our purpose. That was simply done using the unnest_tokens() function from the tidytext library. On a side note, n-gram is a terminology very well known in the world of natural language processing (NLP), and it simply refers to a sequence of n words. If n=1, it is referred to as a unigram, if n=2, it is referred to as a bigram and if n=3, it is referred to as a trigram.\nTo find the top 20 words used per president, each president’s name was filtered, followed by a ‘word’ tokenisation, lower case string detection using the matching pattern ‘[a-z]’, exclusion of stop words (i.e, prepositions and connecting words) from the SMART lexicon of the tidytext library and of course a descending count and slice of the top 20 words. This process resulted as Figure 2.\n\n\n\n\n\nFigure 2: Top 20 words used by each president. A relates to Mandela’s words, B relates to Mbeki’s words, C relates to Zuma’s words, and D relates to Ramaphosa’s words.\n\n\n\n\nIt was quite obvious that the words like government, people, public, south, national, development, african, economic and more would appear as top 20 words for all the presidents. Apart from that, based on the words of Mandela, he really demonstrated a presidency that was committed to public service, inclusivity, community building, and addressing a range of social, economic, and security-related challenges. Mandela’s leadership was marked by a dedication to nation-building, reconciliation, and addressing the needs of the people. Based on the top 20 words from Mbeki, it seemed like his presidency was marked by a commitment to improving public services, addressing social issues, promoting economic growth and development, and addressing significant challenges. His top 20 words were not that meaningful because that is basically what all presidents would want to achieve in a society. Among the top 20 words of Zuma, the ones that really stood out was ‘water’ as during his presidency, South Africa was dealing with some water crisis. Ramaphosa’s top 20 words revealed a lot about his personality. He is a businessman, so his approach as president was very business-oriented. He seemed to have prioritised economic development and social welfare during his term as president. Due to all presidents having the same goals and dealing with more or less similar unresolved issues over the years, similar words were present in their speeches which would make prediction from their speeches quite challenging."
  },
  {
    "objectID": "Assignment1_Tanweer.html#sec-data_var",
    "href": "Assignment1_Tanweer.html#sec-data_var",
    "title": "Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM",
    "section": "2.4 Data Variation Models",
    "text": "2.4 Data Variation Models\nPrior to creating the different data variation, a bag of 200 most frequent words was generated by first tokenised the SONA dataset into words while detecting lower case strings only, and excluding stop words, followed by a top 200 count. Then the dataset was tokenised into sentences after which a sentence ID was mutated to the dataset. The new dataset was now in the form where each row represents a sentence mentioned by a president whereby each sentence would uniquely be identified by the sentence ID. This dataset’s format was the main input for its subsequent variations highlighted below.\n\n2.4.1 Bag of words (BOW) Model\nThe new dataset with sentences was further tokenised into words where again the stop words were filtered out and only lower case strings were detected. The initial bag of frequent words prepared were then inner joined to the latter. That step dropped any words from each sentence with none of the 200 top words. Furthermore, the BOW model was achieved by just counting the number of times each of the top 200 words was used in each sentence. The BOW model was expectedly quite a sparse dataset where each column was represented by the bag of 200 most frequent words. One issue with that dataset was its uniformity (i.e, each president was grouped in order) due to how it was created while using function like group_by(). According to Myrianthous (2021), it is necessary to shuffle the rows of the dataset so that variance is minimised and generalisability can be establised upon modelling. Therefore, the rows were randomly shuffled using the sample() function from base R.\n\n\n2.4.2 TF-IDF on BOW Model\nAs its name suggests, term frequency inverse document frequency (TF-IDF) is an approach where the values for each word in a document is computed through an inverse proportion of the frequency of the word in a particular document to the percentage of words appearance in any documents (Ramos et al. 2003). The mathematical formula for a TF-IDF calculation is given by: \\[ w_d = f_{w,d} \\times log \\left(\\frac{|D|}{f_{w,D}}\\right) \\] where,\n\n\\(w\\) = word;\n\\(D\\) = document collection or corpus;\n\\(d\\) = individual document, where \\(\\{d \\in D\\}\\);\n\\(f_{w,d}\\) = frequency of \\(w\\) in \\(d\\);\n\\(|D|\\) = size of the corpus (Ramos et al. 2003).\n\nThe \\(log \\left(\\frac{|D|}{f_{w,D}}\\right)\\) lies between 1 and a very small constant \\(c\\), implies that \\(w\\) is relatively common over the entire collection of document while still holding some validity throughout \\(D\\) (Ramos et al. 2003). So, the second data variation was created, by applying TF-IDF to the previous BOW model (annotated as TF-IDF (BOW) in subsequent sections), and in R it is as simple as to just utilise the bind_tf_idf() function from the tidytext library. Finally, once again, the rows of the dataset were randomly shuffled as argued in Section 2.4.1.\n\n\n2.4.3 Dealing with Imbalanced Data\n\n\n\nTable 1 tabulated the totality of sentences appeared per president for both BOW and TF-IDF (BOW)\n\n\n\n\nTable 1: Extract of the number of sentences appeared per president for both BOW and TF-IDF (BOW) models.\n\n\n\nMandela\nMbeki\nRamaphosa\nZuma\n\n\n\n\nBOW\n1507\n2249\n2088\n2334\n\n\nTF-IDF (BOW)\n5498\n10420\n7899\n8082\n\n\n\n\n\n\nmodels and we observed that in both cases the data was very imbalanced. The 2 most simplest ways to fix this problem is by applying up-sampling or down-sampling. Up-sampling is when the data in the minority classes are randomly duplicated or recreated to match the number of sample in that majority class. For instance, an up-sampled (US) version of the TF-IDF (BOW) (annotated as US TF-IDF (BOW)) model would unify all the entries of the other presidents to 10420, as Mbeki had the highest number of sentences. Similarly, down-sampling is when the data in the majority classes are randomly omitted to match the number of sample in that minority class. For instance, a down-sampled (DS) version of the BOW model (annotated as DS BOW) would unify all the entries of the other presidents to 1507, as Mandela had the lowest number of sentences. Both up-sampling and down-sampling have their pros and cons. For example, up-sampling can cause over-fitting but down-sampling can under-fitting due to limited information for model training. So, on the 2 model variations that we already had as explained in Section 2.4, up-sampling and down-sampling have been applied on each. That led us into having 6 data model variations in total."
  },
  {
    "objectID": "Assignment1_Tanweer.html#sec-PM",
    "href": "Assignment1_Tanweer.html#sec-PM",
    "title": "Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM",
    "section": "2.5 Predictive Models",
    "text": "2.5 Predictive Models\nThe main intention behind this research was to be able to predict the presidents in question based on their speeches. Therefore, 3 robust machine learning models such as the deep feedforward neural network (NN), multinomial logistic regression (MLR) and support vector machine (SVM) were picked to execute the task. The following sections concisely explain each algorithm. Although hyper-parameter tuning is highly recommended, it was not performed due to time constraint except for the MLR model. Therefore, the hyper-parameters were chosen through multiple manual execution and evaluation of the models (or a manual hyper-parameter tuning). Throughout the modelling process, each data variation was split into 70% training set and 30% test set. The training set was scaled separately and then the test set was scaled but based on the unscaled training data’s mean and standard deviation. Additionally, for the NN application only, the dataset had to undergo through the process of one hot encoding using the to_categorical function, in preparation to be used with the keras library. Note that the explanation of each algorithm below was applied to the 6 data variations derived as explained in Section 2.4.\n\n2.5.1 Deep feedforward Neural Network (NN)\nIn a nutshell, NN functions just like our brain where it takes a certain number of inputs and assign weights to each while distributing those in a network of intermediary outputs, assign further weights and repeat this process if necessary, until the final collection of decision/output is obtained. More formally, it contains of a layer of input with all the predictor variables, number of neurons for each hidden layer and the output layer containing the possible predictions. There are multiple activation functions which can be applied in the hidden layers and output layer. Initially, the NN models were executed using the rectified linear unit (ReLU) in the hidden layers but was found that the hyperbolic tanh activation function performed better. Mathematically, the hyperbolic tanh function can be expressed as: \\[ f(x) = \\frac{2}{(1+e^{-2x})}-1 \\] (Sharma, Sharma, and Athaiya 2017). Graphically, it is a sigmoid function symmetric around the origin and ranges from -1 to 1. Since we were dealing with a multi-class classification, the softmax was the appropriate activation function for the output layer. Softmax is a generalised sigmoidal function (Almurieb and Bhaya 2020) or can be seen as multiple sigmoid functions combined (Sharma, Sharma, and Athaiya 2017). Sigmoid function is generally used for binary classification as the function returns a value between 0 and 1 (Sharma, Sharma, and Athaiya 2017). Thus, its generalisability (the so-called softmax) makes it more flexible which in turn allows multi-class classification. Without any explanation, the equation for the softmax function is defined as follows: \\[ \\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{l=1}^d e^{x'_l}} \\], and more information can be found in Almurieb and Bhaya (2020).\nMoreover, the deep feedforward NN was applied using the keras library with the following steps:\n\nCreate an empty model using the keras_model_sequential() function.\nDefine the model:\n\nThe architecture contained 200 inputs, with a hidden layer of 500 units/neurons and tanh activation function followed by a dropout regulariser at a 0.1 rate. The second hidden layer had 300 units/neurons with the same activation function and again the dropout regulariser at the same rate. Finally, the output layer contained 4 unitswith a softmax activation function.\n\nCompile the model using the compile() function:\n\nAt a bare minimum, the categorical crossentropy was chosen as loss function with the ‘adam’ optimiser at a learning rate of 0.01. The metric to be returned was selected to ‘accuracy’.\n\nTrain the model using the fit() function:\n\nThe algorithm was set to iterate for 100 epochs in small batches of 30. The validation set used was 20% with shuffle set to true for more generalisability.\n\nEvaluate model using the evaluate() function on the test set.\n\n\n\n2.5.2 Multinomial Logistic Regression (MLR)\nLogistic regression model are ideal for binary classification as it normally assumes the categorical response variable as success (1) or failure (0) (El-Habil 2012). A slight modification to this algorithm gives rise to the so-called multinomial logistic regression. Briefly, MLR revolves around the same idea of logarithm of the odds called logit just as logistic regression, but simply extended to handle multiple explanatory variables (El-Habil 2012). The MLR was quite simply implemented using the glmnet() function from the glmnet library, while invoking a lasso (L1) regularisation to minimise overfitting. Lasso regularisation can shrunk some coefficients exactly to 0 indicating their non-statistically significant nature of that predictor variable. Therefore, it automatically excludes them from the model. To find the optimal \\(\\lambda\\), a 10-fold cross validation was performed using the cv.glmnet() function.\n\n\n2.5.3 Support Vector Machine (SVM)\nIn short, SVM sprouts from support vector classifiers (SVC) where the main idea is to separate various classes using decision boundaries whereby for a binary classification the SVC typically contains 2 margins. Of course, this can be uplifted to multi-class classification where the SVC would contain N number of margins/hyperplanes. SVM aims to maximise the margin so that the concept of variance-bias trade off is satisfied. That is, while a tight margin is subject to overfitting, a wide one introduces more generalisability. As opposed to SVC, SVM has an additional function which implement a kernel trick, facilitating non-linear classification by enlarging the feature space and improving computational efficiency (James et al. 2013). SVM was applied using the svm() function from the e1071 library. During the implementation of SVM, after several manual testing and evaluation we established on maintaining a cost of 1 with a radial kernel trick. According to Patle and Chouhan (2013), radial kernel is one of the most popular kernal functions which informally speaking, adds a ‘bump’ around each data points. Mathematically, the kernel function is represented as: \\[K(x,x_i) = e^{-\\gamma \\left\\|x-x_i\\right\\|^2}\\], where \\(\\gamma > 0\\) (Patle and Chouhan 2013)."
  },
  {
    "objectID": "Assignment1_Tanweer.html#performance-evaluation-metrics",
    "href": "Assignment1_Tanweer.html#performance-evaluation-metrics",
    "title": "Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM",
    "section": "2.6 Performance Evaluation Metrics",
    "text": "2.6 Performance Evaluation Metrics\nAlthough all the models were evaluated using the conventional training and test accuracy, the imbalanced datasets were evaluated using the Matthew’s Correlation Coefficient (MCC). Because, according to Chicco and Jurman (2020), MCC is the most reliable metric when it comes to imbalance data, as it takes this specific nature into account. MCC is typically performed on binary classification and is quite straight forward. However, although MCC is still possible to compute for multi-class classification, its mathematical formulation is relatively slightly complicated. Let \\(C\\) be the confusion matrix for class \\(k\\), mathematically, the MCC for multi-class classification is computed as follows: \\[ MCC = \\frac{c \\times s - \\sum_k^K p_k \\times t_k }{\\sqrt{(s^2-\\sum_k^K p_k^2)(s^2-\\sum_k^Kt_k^2)}} \\] where,\n\n\\(c = \\sum_k^K C_{kk}\\), the total number of correct predictions (sum of diagonal);\n\\(s = \\sum_i^K \\sum_j^K C_{ij}\\), the total number of observations;\n\\(p_k = \\sum_i^K C_{ki}\\), the number of times the class k was predicted (column total);\n\\(t_k = \\sum_i^K C_{ik}\\), the number of times the class k truly occurred (row total) (Grandini, Bagli, and Visani 2020).\n\nThe coefficient ranges from -1 to 1 and is evaluated as the standard correlation matrix. In R, it was simply calculated using the mcc() function from the mltools library."
  },
  {
    "objectID": "Assignment1_Tanweer.html#inter-model-comparison",
    "href": "Assignment1_Tanweer.html#inter-model-comparison",
    "title": "Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM",
    "section": "3.1 Inter-Model Comparison",
    "text": "3.1 Inter-Model Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe implementation of the 3 machine learning algorithms discussed in Section 2.5 on each of the 6 data variations (see Section 2.4), resulted in 18 outcomes as shown in Table 2. A first glimpse on the overall performance metrics, revealed extremely poor accuracies. In fact all the test accuracies were\n\n\n\n\nTable 2: Model performance in predicting the presidents from their speeches using 200 predictor variables. Metrics reported on 18 different models are MCC, training accuracy and test accuracy. Empty cells for MCC is due to application irrelevancy- classes are fully balanced.\n\n\n\n\n\n\n\n\nModel\nMCC\nTraining Accuracy\nTest Accuracy\n\n\n\n\nNNtanh+softmax: Original BOW\n-0.007\n0.778\n0.242\n\n\nNNtanh+softmax: US BOW\n-\n0.782\n0.247\n\n\nNNtanh+softmax: DS BOW\n-\n0.805\n0.265\n\n\nNNtanh+softmax: Original TF-IDF (BOW)\n0.001\n0.349\n0.244\n\n\nNNtanh+softmax: US TF-IDF (BOW)\n-\n0.335\n0.252\n\n\nNNtanh+softmax: DS TF-IDF (BOW)\n-\n0.332\n0.249\n\n\n\n\n\n\n\n\nMLRL1: Original BOW\n-0.055\n0.501\n0.461\n\n\nMLRL1: US BOW\n-\n0.499\n0.435\n\n\nMLRL1: DS BOW\n-\n0.482\n0.456\n\n\nMLRL1: Original TF-IDF (BOW)\n-0.026\n0.404\n0.385\n\n\nMLRL1: US TF-IDF (BOW)\n-\n0.38\n0.373\n\n\nMLRL1: DS TF-IDF (BOW)\n-\n0.382\n0.351\n\n\n\n\n\n\n\n\nSVMradial: Original BOW\n0.286\n0.693\n0.471\n\n\nSVMradial: US BOW\n-\n0.704\n0.473\n\n\nSVMradial: DS BOW\n-\n0.714\n0.455\n\n\nSVMradial: Original TF-IDF (BOW)\n0.155\n0.436\n0.393\n\n\nSVMradial: US TF-IDF (BOW)\n-\n0.412\n0.39\n\n\nSVMradial: DS TF-IDF (BOW)\n-\n0.42\n0.363\n\n\n\n\n\n\nbelow 50% and the MCCs also were quite low. In addition, only 5 training accuracies were above 70%. For the unbalanced data, SVM has definitely demonstrated as best candidate due to its relatively high MCC (0.286). Although, the best performer according to training accuracy kept on altering among each group of models, the models applied on the DS TF-IDF (BOW) were consistently the worst throughout each group of models. Regarding the test accuracy, the family of NN showed very poor results and for both the MLR and SVM, the test accuracy were around the same values. Throughout all the models applied, NN was definitely the poorest performer."
  },
  {
    "objectID": "Assignment1_Tanweer.html#intra-model-comparison",
    "href": "Assignment1_Tanweer.html#intra-model-comparison",
    "title": "Prediction of South African Presidents from their SONA Speeches using NN, MLR & SVM",
    "section": "3.2 Intra-Model Comparison",
    "text": "3.2 Intra-Model Comparison\nAmong the family of NN models, the NN applied on the original BOW and the original TF-IDF (BOW) were both extremely poor with MCCs of -0.007 and 0.001, respectively. Albeit, the NN applied on the DS BOW having the highest training accuracy of 0.805, its test accuracy (0.265) was the poorest. This is a very good example showing the complexity of the model being high, biased and therefore obtaining a very good training accuracy. Due to its poor biased-variance trade off, in this case overfitting on the training set, the test error became really high on the unseen test data. Surprisingly, the NN model applied on the DS TF-IDF with the lowest training accuracy (0.332) had relatively one of the highest test accuracy (0.249).\nAmid the family of MLR models, the MLR applied on the original BOW and the original TF-IDF (BOW) were even worse with MCCs of -0.055 and -0.026, respectively. However, the MLR applied on the original BOW had relatively the largest training and test accuracy of 0.501 and 0.461, respectively. Furthermore, the MLR on the US TF-IDF (BOW) had the lowest performance with a training accuracy of 0.38. It could also be observed that the training and test accuracies were fairly consistent among each other, meaning that the biased-variance trade off was quite balanced.\nLast but not least, within the family of SVM models, the SVM applied on the original BOW had a better MCC (0.286) than the original TF-IDF (BOW) one (0.155). SVM onto the DS BOW dataset highlighted the highest training accuracy of 0.714 which pertained to a test accuracy of 0.455. The lowest training and test accuracy were 0.42 and 0.363, respectively, for the SVM applied on DS TF-IDF (BOW). The non TF-IDF dataset seemed to have performed better both in terms of training and test accuracy."
  }
]